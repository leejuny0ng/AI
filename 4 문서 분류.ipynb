{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOaP/zwi2NB1q5MHB63Crtb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leejuny0ng/AI/blob/main/4%20%EB%AC%B8%EC%84%9C%20%EB%B6%84%EB%A5%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQLavvmxhrpS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "문서 분류 (Document Classification)"
      ],
      "metadata": {
        "id": "xd7wm7qbidiR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p8wy47KUii3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 준비\n",
        "- 문서분류에 필요한 데이터는 scikit-learn이 제공하는 20개의 주제를 가지는 뉴스그룹 데이터를 사용\n",
        "- 텍스트는 CounterVectorizer를 거쳐 DTM으로 변환\n",
        "- DTM은 문서에 등장하는 단어들을 빈도 수 별로 표현한 행렬"
      ],
      "metadata": {
        "id": "slT4Ydy_ijQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "news = fetch_20newsgroups()\n",
        "# 뉴스 받아옴\n",
        "\n",
        "x = news.data\n",
        "y = news.target\n",
        "\n",
        "cv = CountVectorizer()\n",
        "x = cv.fit_transform(x)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw5reFXhi16H",
        "outputId": "342b5ea3-87db-4362-b14f-6554b0a35209"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7919, 130107) (7919,) (3395, 130107) (3395,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKQEOuuEjQHu",
        "outputId": "f14e4eb7-4412-414b-fdfa-f4f3850d353a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 56979)\t2\n",
            "  (0, 85354)\t1\n",
            "  (0, 114688)\t1\n",
            "  (0, 111322)\t1\n",
            "  (0, 114731)\t1\n",
            "  (0, 90379)\t1\n",
            "  (0, 118983)\t1\n",
            "  (0, 89362)\t2\n",
            "  (0, 76032)\t1\n",
            "  (0, 123292)\t1\n",
            "  (0, 65798)\t2\n",
            "  (0, 114579)\t1\n",
            "  (0, 114455)\t11\n",
            "  (0, 90686)\t1\n",
            "  (0, 68766)\t3\n",
            "  (0, 115475)\t9\n",
            "  (0, 32311)\t4\n",
            "  (0, 66608)\t3\n",
            "  (0, 73201)\t1\n",
            "  (0, 37565)\t2\n",
            "  (0, 90252)\t1\n",
            "  (0, 128402)\t1\n",
            "  (0, 62221)\t1\n",
            "  (0, 94362)\t1\n",
            "  (0, 78955)\t1\n",
            "  :\t:\n",
            "  (0, 108267)\t1\n",
            "  (0, 29782)\t1\n",
            "  (0, 84436)\t1\n",
            "  (0, 106317)\t1\n",
            "  (0, 40981)\t1\n",
            "  (0, 3436)\t1\n",
            "  (0, 13651)\t2\n",
            "  (0, 99754)\t1\n",
            "  (0, 106001)\t1\n",
            "  (0, 43122)\t1\n",
            "  (0, 12131)\t1\n",
            "  (0, 939)\t1\n",
            "  (0, 96668)\t2\n",
            "  (0, 103802)\t1\n",
            "  (0, 77457)\t2\n",
            "  (0, 79224)\t1\n",
            "  (0, 78997)\t1\n",
            "  (0, 30027)\t1\n",
            "  (0, 86455)\t2\n",
            "  (0, 30525)\t1\n",
            "  (0, 40824)\t2\n",
            "  (0, 72327)\t1\n",
            "  (0, 51744)\t1\n",
            "  (0, 94312)\t1\n",
            "  (0, 20046)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "scikit-learn을 이용한 문서 분류"
      ],
      "metadata": {
        "id": "fnYPRhpJoYHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "1OIqPeRmokc-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ra6HmQWoqqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "로지스틱 회귀(logistic regression)\n",
        "- 로지스틱 회귀는 이름에 회귀라는 단어가 들어가지만, 가능한 클래스가 2개인 이진 분류를 위한 모델\n",
        "- 로지스틱 회귀의 예측함수 정의\n",
        "- 로지스틱 회귀 모델은 선형 회귀 모델에서 시그모이드 함수를 적용\n",
        "- 로지스틱 회귀의 학습 목표는 다음과 같은 목적함수를 최소화하는 파라미터 w를 찾는 것\n",
        "\n",
        "\n",
        "- Logistic Regression은 특성상 다중 분류에는 적합하지 않음"
      ],
      "metadata": {
        "id": "9I1hHomdosiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "LR = LogisticRegression()\n",
        "LR.fit(x_train, y_train)\n",
        "pred = LR.predict(x_test)\n",
        "# 예측 가능\n",
        "acc = accuracy_score(pred, y_test)\n",
        "# 예측한 pred와 y 사이의 얼만큼의 approach가 나오는지 확인\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJ5vDaiCpouF",
        "outputId": "a2b4ac24-9b52-4a4a-adef-7f17f4ad65bc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8668630338733432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hkNOvzKVp8Qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 서포트 벡터 머신(Support Vector Machines)\n",
        "\n",
        "- 회귀, 분류, 이상치 탐지 등에 사용되는 지도학습 방법\n",
        "- 클래스 사이의 경계에 위치한 데이터 포인트를 서포트 벡터(support vector)라고 함\n",
        "- 각 서포트 벡터가 클래스 사이의 결정 경계를 구분하는데 얼마나 중요한지를 학습\n",
        "- 각 서포터 벡터 사이의 마진이 가장 큰 방향으로 학습\n",
        "- 서포트 벡터까지의 거리와 서포트 벡터의 중요도를 기반으로 예측을 수행"
      ],
      "metadata": {
        "id": "NH-q1FwfqLba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "\n",
        "SVM = svm.SVC(kernel='linear')\n",
        "# svm의 SVC > 분류\n",
        "SVM.fit(x_train, y_train)\n",
        "# x train과 ytrain fit\n",
        "pred = SVM.predict(x_test)\n",
        "acc = accuracy_score(pred, y_test)\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC6Jz8lNqr16",
        "outputId": "615a67bf-7061-420a-f740-3e2471ce6e3b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8191458026509573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c9ikSx3JrPwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 나이브 베이즈 분류기 (Naive Bayes Classification)\n",
        "\n",
        "- 베이즈 정리를 적용한 확률적 분류 알고리즘\n",
        "- 모든 특성들이 독립임을 가정 (naive 가정)\n",
        "- 입력 특성에 따라 3개의 분류기 존재\n",
        "\n",
        "  - 가우시안 나이브 베이즈 분류기\n",
        "  - 베르누이 나이브 베이즈 분류기\n",
        "  - 다항 나이브 베이즈 분류기"
      ],
      "metadata": {
        "id": "jl_3TSCerUMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DTM(문서 단어행렬)을 이용한 나이브 베이즈"
      ],
      "metadata": {
        "id": "9pK-ZiH-rscT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "NB = MultinomialNB()\n",
        "NB.fit(x_train, y_train)\n",
        "pred = NB.predict(x_test)\n",
        "acc = accuracy_score(pred, y_test)\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buecM2Cerr8c",
        "outputId": "15d87aa2-65d7-4fb2-dbdd-ade7faae9ebf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8044182621502209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z6p7wN1dr34S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF를 이용한 정확도 향상"
      ],
      "metadata": {
        "id": "a8QujrqWsLFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf = TfidfTransformer()\n",
        "x_train_tf = tfidf.fit_transform(x_train)\n",
        "x_test_tf = tfidf.fit_transform(x_test)\n",
        "\n",
        "NB.fit(x_train_tf, y_train)\n",
        "pred = NB.predict(x_test_tf)\n",
        "acc = accuracy_score(pred, y_test)\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhzERYSBsN5A",
        "outputId": "558f1a8d-a567-4b59-a062-2694e2e6d54b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8141384388807069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "결정 트리(Decision Tree)\n",
        "\n",
        "- 분류와 회귀에 사용되는 지도 학습 방법\n",
        "- 데이터 특성으로부터 추론된 결정 규칙을 통해 값을 예측\n",
        "- if-then-else 결정 규칙을 통해 데이터 학습\n",
        "- 트리의 깊이가 깊을수록 복잡한 모델\n"
      ],
      "metadata": {
        "id": "FKo7CEYBsh78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "DT = DecisionTreeClassifier()\n",
        "DT.fit(x_train, y_train)\n",
        "pred = DT.predict(x_test)\n",
        "acc = accuracy_score(pred, y_test)\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96-6Se5MsQTA",
        "outputId": "3da47e68-4562-497a-d908-8c33ecdab84d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6329896907216495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dXkda-s9s6Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost\n",
        "- 트리 기반의 앙상블 기법 (여러개의 개별 모델을 결합하여 하나의 강력한 모델을 구현하는 기법)\n",
        "- 분류에 있어서 다른 알고리즘보다 좋은 예측 성능을 보여줌\n",
        "- XGBoost는 GBM 기반이지만, GBM의 단점인 느린 수행 시간과 과적합 규제 부재 등의 문제를 해결\n",
        "- 병렬 CPU 환경에서 빠르게 학습 가능"
      ],
      "metadata": {
        "id": "bZ9U56rQs95C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgb = XGBClassifier(n_estimators=30, learning_rate=0.05, max_depth=3)\n",
        "xgb.fit(x_train, y_train)\n",
        "pred = xgb.predict(x_test)\n",
        "acc = accuracy_score(pred, y_test)\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGcjZabstdyD",
        "outputId": "4976cfe0-f5cd-4764-cf33-53385411dbc3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7036818851251841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nOifSsF0tfhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "교차 검증\n",
        "- 일반 검증은 학습 데이터가 테스트 데이터로 사용되지 않음\n",
        "- 교차 검증은 데이터를 n개의 집합으로 나누어 정확도를 계산해 학습 데이터로 사용된 데이터도 테스트 데이터로 사용\n",
        "- 교차 검증을 사용하면 일반 검증보다 모델의 일반화가 잘 되어있는지 평가 가능\n",
        "- 앞서 구성한 나이브 베이즈 모델을 교차 검증"
      ],
      "metadata": {
        "id": "M0rVjXpWtn4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(NB, x, y, cv=5)\n",
        "# 나이브 베이즈, x, y, 5개 집합으로 구분\n",
        "print(scores, scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oP8Ciybbt6tL",
        "outputId": "39bc04c9-09d4-4922-a997-dcfc85f92a8c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.83870968 0.83826779 0.82368537 0.83031374 0.83642794] 0.833480903927519\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qzzO7Ex3t9_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 교차 검증을 통해 일반 검증보다 좀 더 일반화된 모델 생성 가능\n",
        "- 교차 검증은 일반 검증에 비해 n번 검증을 해 비용이 더 많이 소요"
      ],
      "metadata": {
        "id": "0dZJ8jx6uAFS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TlrK_BiouIxG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 정밀도와 재현률\n",
        "* 정밀도(precision)는 양성 클래스(정답)으로 예측한 샘플이 양성 클래스일 확률을 의미\n",
        "* 모델이 얼마나 양성 클래스를 잘 예측하는지를 나타냄\n",
        "* 재현률(recall)은 양성 클래스인 샘플에서 모델이 양성 클래스로 예측한 샘플 비율을 의미하며, 모델이 얼마나 실제 상황을 재현하는지를 나타냄\n",
        "* 정밀도와 재현율의 가중조화평균인 F1-score라는 지표는 정확도에 비해 더 효과적인 모델 분석 지표로 알려져 있음\n",
        "* 직접 계산할 수도 있으나, scikit-learn은 이를 편리하게 계산해주는 함수를 제공\n",
        "\n"
      ],
      "metadata": {
        "id": "W6n8RlNDuJxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 다중 클래스 분류 문제에서 정밀도와 재현률을 계산할 때는 클래스간의 지표를 어떻게 합칠지 지정 필요\n",
        "\n",
        "  * None - 클래스간 지표를 합치지 말고 그대로 출력\n",
        "  * micro - 정밀도와 재현률이 같음, 이로 인해 f1-score도 정밀도, 재현률과 동일\n",
        "  * macro - 클래스간 지표를 단순 평균한 값\n",
        "  * weighted - 클래스간 지표를 가중 평균한 값\n",
        "  "
      ],
      "metadata": {
        "id": "NkE9E7YPymYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "precision = precision_score(pred, y_test, average='micro')\n",
        "recall = recall_score(pred, y_test, average='micro')\n",
        "f1 = f1_score(pred, y_test, average='micro')\n",
        "\n",
        "print(precision, recall, f1)\n",
        "# micro로 했기 때문에 정밀도와 재현률 동일"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkQt7lPMyukE",
        "outputId": "48ac5693-938f-4cee-ea57-dafd2bf2858b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7036818851251841 0.7036818851251841 0.7036818851251841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision = precision_score(pred, y_test, average='macro')\n",
        "recall = recall_score(pred, y_test, average='macro')\n",
        "f1 = f1_score(pred, y_test, average='macro')\n",
        "\n",
        "print(precision, recall, f1)\n",
        "# macro"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mluYS-tKzeLO",
        "outputId": "b8ec9667-2c75-4c89-bf99-27c382b59173"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7014289333127905 0.7378283939295427 0.7126687906783633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-3DTzS1pzhoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 그리드 검색을 이용한 파라미터 최적화\n",
        "- 그리드 검색을 사용하면 분류기에 사용하는 파라미터 최적화 가능\n",
        "- 그리드 검색을 통해 앞서 구성한 나이브 베이즈 모델의 'alpha' 파라미터를 최적화시키는 예제\n",
        "\n",
        "- estimator : 사용 모델 객체\n",
        "- param_grid : 사용 객체) 지정 파라미터 리스트로 구성된 딕셔너리\n",
        "- scoring : 최적화하고자 하는 성능 지표\n",
        "- cv : 교차 검증 분할 개수"
      ],
      "metadata": {
        "id": "CaNhq5jTzkGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "GS = GridSearchCV(estimator=NB, param_grid={'alpha': [0.001, 0.01, 0.1, 1.0, 10.0]}, scoring='accuracy', cv=10)\n",
        "GS.fit(x, y)\n",
        "\n",
        "print(GS.best_score_)\n",
        "print(GS.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QA5TCk-z4eQ",
        "outputId": "40d401e8-f96f-48bb-bdc9-57a4d7b7eadf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8897820965842167\n",
            "{'alpha': 0.001}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lrpsf4Cd0KRe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}